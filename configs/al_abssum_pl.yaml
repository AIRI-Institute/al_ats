output_dir: './workdir/run_active_learning'
hydra:
  run:
    dir: ${output_dir}/${now:%Y-%m-%d}/${now:%H-%M-%S}_${seed}_${to_string:${acquisition_model.name}}
  sweep:
    dir: ${output_dir}/${now:%Y-%m-%d}/${now:%H-%M-%S}_${seed}_${to_string:${acquisition_model.name}}
    subdir: ${hydra.job.num}

seed: 42
cuda_device: 0
cache_dir: ./${output_dir}/cache_${seed}_${to_string:${acquisition_model.name}}
cache_model_and_dataset: False
framework: ???
task: 'abs-sum'
offline_mode: False

data:
    dataset_name: 'xsum'
    text_name: 'document'
    label_name: 'summary'
    labels_to_remove:
    path: 'datasets'
    train_size_split: 0.9
    filter_quantiles:
    tokenizer_name: ${acquisition_model.name}

acquisition_model:
    type: ${task}
    name: 'facebook/bart-base'
    tokenizer_max_length: 1024
    num_labels:
    classifier_dropout: 0.
    exists_in_repo: True  # whether the model exists in HF models repo
    path_to_pretrained:  # required if the model does not exist in HF models repo
    training:
        dev_size: 0.
        shuffle_dev: False
        freeze_embedder: False
        batch_size_args:
            batch_size: 16
            eval_batch_size: 70
            generate_batch_size: 50
            min_num_gradient_steps: 350
            adjust_batch_size: True
            adjust_num_epochs: True
            min_batch_size: 4
        trainer_args:
            num_epochs: 6
            patience: ${get_patience_value:${acquisition_model.training.dev_size}}
            grad_clipping: 0.28
            serialization_dir: ./output/${to_string:${acquisition_model.name}}_${seed}_${now:%Y-%m-%d}_${now:%H-%M-%S}/acquisition
            validation_metric: ${framework.validation_metric_abssum}
            evaluation_strategy: 'no' # for transformers wrapper, 'epoch' or 'steps' or 'no'
            eval_metrics: ["sacrebleu"]
            fp16:
                training: True
                evaluation: False
            accumulation:
                gradient_accumulation_steps: 1
                eval_accumulation_steps: 1
            generation_max_length:  # if None, equals max len among summaries in the train sample
            generation_num_beams:
        optimizer_args:
            weight_decay: 0.028
            lr: 2e-5
        scheduler_args:
            warmup_steps_factor: 0.1
            use_adafactor: False

successor_model:

target_model:
    type: ${task}
    name: 'facebook/bart-base'
    tokenizer_max_length: 1024
    num_labels:
    classifier_dropout: 0.
    exists_in_repo: True  # whether the model exists in HF models repo
    path_to_pretrained:  # required if the model does not exist in HF models repo
    training:
        dev_size: 0.
        shuffle_dev: False
        freeze_embedder: False
        batch_size_args:
            batch_size: 16
            eval_batch_size: 70
            min_num_gradient_steps: 350
            adjust_batch_size: True
            adjust_num_epochs: True
            min_batch_size: 4
        trainer_args:
            num_epochs: 6
            label_smoothing_factor: 0.0
            patience: ${get_patience_value:${target_model.training.dev_size}}
            grad_clipping: 0.28
            serialization_dir: ./output/${to_string:${target_model.name}}_${seed}_${now:%Y-%m-%d}_${now:%H-%M-%S}/target
            validation_metric: ${framework.validation_metric_abssum}
            evaluation_strategy: 'no' # for transformers wrapper, 'epoch' or 'steps' or 'no'
            eval_metrics: ["sacrebleu"]
            fp16:
                training: True
                evaluation: False
            accumulation:
                gradient_accumulation_steps: 1
                eval_accumulation_steps: 1
            generation_max_length:  # if None, equals max len among summaries in the train sample
            generation_num_beams:
        optimizer_args:
            weight_decay: 0.028
            lr: 2e-5
        scheduler_args:
            warmup_steps_factor: 0.1
            use_adafactor: False

#ensemble_model_2: ???
#ensemble_model_3: ???
#discriminator_model: ???

al:
    strategy: 'seq_score'
    num_queries: 15
    init_p_or_n: 50
    step_p_or_n: 50
    gamma_or_k_confident_to_save: 10000
    T: 0.
    sampling_type: random
    iters_to_recalc_scores: "no"
    evaluate_query: True
    strategy_kwargs:
        enable_dropout: False
        mc_iterations: 10
        discriminator_test_size:
        filtering_mode:  # 'score', 'percentile', 'rouge1', 'rouge2', 'rougeL', 'sacrebleu'
        uncertainty_mode: 'absolute'
        filtering_aggregation: 'precision'
        uncertainty_threshold: 1.
        var_metric: bleu
#        ensemble_model_2: ${ensemble_model_2}
#        ensemble_model_3: ${ensemble_model_3}
#        discriminator_model: ${discriminator_model}
        rouges_dir: output_rouges
        queries_path:
        embeddings_model_name: "roberta-base"
        text_name: "document"
        label_name: "summary"
        obj_id_name: "id"
        subsample_ratio: 10


tracin:
    use: False


#al_strategy: ???

defaults:
  - framework: transformers # 'allennlp' or 'transformers'
#  - ensemble_model_2: pegasus-large
#  - ensemble_model_3: prophetnet-large
#  - discriminator_model: electra
#  - al_strategy: lc # lc, random, mahalanobis, nuq, ddu, logits_lc